{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import math\n",
    "from fancy_einsum import einsum\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import EasyTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:  2.0.1\n",
      "Torch device:  mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version: \", torch.__version__)\n",
    "\n",
    "# Default device\n",
    "device = \"cpu\"\n",
    "\n",
    "# Check Pytorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "# Check Pytorch has access to CUDA\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Set device to GPU if available\n",
    "torch.device(device)\n",
    "print(\"Torch device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2_xl = EasyTransformer.from_pretrained(\"gpt2-xl\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the point of a transformer?\n",
    "\n",
    "**Key takeaway**: Transformers are *sequence modelling engines*. At each sequence position, it does the same processing in parallel. It moves information between positions with attention, and conceptually can take a sequence of arbitrary length (not actually true, see later)\n",
    "\n",
    "**Key feature**: They generate text! You feed in language, and the model generates a probability distribution of next tokens, which you can repeatedly sample over to generate more text.\n",
    "There are many different types of transformers but this notebook will focus on GPT-2 style transformers.\n",
    "\n",
    "\n",
    "### How is the model trained?\n",
    "\n",
    "You give it a bunch of text and it is trained to predict the next token. Importantly, if you give a model 100 tokens in a sequence, it predicts the next token for *each* prefix, i.e. it produces 100 predictions. This is unintuitive at first since we really just want the last token prediction. But in reality, it makes training more efficient because you can use 100 bits of feedback instead of just one. We make the transformer have *causal attention* in order to take advantage of all this extra information. A core point is that it can only move information forwards in a sequence i.e. the prediction of what comes after token 50 is a function of the first 50 tokens and *not* of token 51. (Jargon: *autoregressive*: the transformer's output is fed back into its input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens - Transformer Inputs\n",
    "\n",
    "*Core point*: Input is language e.g. a sequence of characters, strings etc. \n",
    "\n",
    "### How do we convert language to vectors?\n",
    "\n",
    "ML models operate on vectors, not raw language, so we need to convert it somehow. \n",
    "\n",
    "### Idea 1: integers to vectors\n",
    "\n",
    "Convert to integers (in a fixed range) -> integers to vecotrs. This is essentially a lookup table, called an embedding. \n",
    "\n",
    "> Lookup tables are equivalent to multiplying a fixed matrix by the one-hot encoded vector. Think about it, the index that is 1 in the one-hot vector will pick out the corresponding column of the matrix.\n",
    "\n",
    "\n",
    "> Jargon: *Encoding* is the process of converting data into a different format.\n",
    "\n",
    "> Jargon: *Embedding* is the process of pairing objects (such as words or entities) with vectors of real numbers. More used in a machine learning context.\n",
    "    \n",
    "> Jargon: *One-hot encoding* e.g. We map numbers from 1 to 100, to a 100 dimensional vector, with a 1 in the position of the number and 0s everywhere else. This lets you think about each integer independently.\n",
    "\n",
    "Dimensions = things that vary independently. each input has its  own dimension, so each input can be thought of independently, we don't bake in any assumptions about the relationship between inputs.\n",
    "\n",
    "But what if we want to encode structure into the embedding? In some contexts, this structural similarity is important e.g. if you were to encode colors, you might want to encode the fact that red and orange are similar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens: Language to sequence of integers\n",
    "\n",
    "*Core idea*: We need a model that can deal with arbitrary text.\n",
    "\n",
    "*Key properties*: there should be a conversion to integers *and* these integers should be in a bounded range\n",
    "\n",
    "Idea: Form a vocaulary i.e. a set of words known and used by someone or some people \n",
    "\n",
    "Idea 1: We could form a dictionary, where you take in language and you look up the index in a dictionary. The problem here is that it can't cope with arbitrary text e.g. URLs, punctuation, etc. or mispellings.\n",
    "\n",
    "Idea 2: We could just do characters, our vocab = 256 ASCII characters. The main problem here is that it loses some structure of language - some sequences of characters are more meaningful than others.\n",
    "\n",
    "The word *language* is more meaningful than *asdfjkl* - we want *language* to be a token and *asdfjkl* not to be. Not to mention this is a more efficient use of our vocab.  \n",
    "\n",
    "#### What Actually Happens?\n",
    "\n",
    "A process called tokenization (forming a vocabulary) happens. In the context of GPT-2, it uses something called Byte-Pair Encodings. It's super weird. \n",
    "\n",
    "The tokenizer has a dictionary of tokens. These tokens are sequences of characters. If we print out the vocabulary of a tokenizer we see seemingly random sequences of characters.\n",
    "\n",
    "Ġ ~ means begins with a space, tokens with a leading space are different then those that are not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding\n",
    "\n",
    "**Core idea**: An algorithm for segmenting text into tokens.\n",
    "\n",
    "**Key properties**: Encouraged to use frequent words and frequent subwords e.g. -ing, -ed, -s, etc. e.g. unlikeliest -> un-, likely, -est\n",
    "Another interpretation is that the most frequent words are represented as a single token while more rare words are broken down into two or more subword tokens.\n",
    "\n",
    "Instead of breaking up words at every whitespace or breaking up at every character, we let the data tell us how to tokenize.\n",
    "\n",
    "> Jargon: Subword tokenization: tokens can be parts of words as well as whole words. This is a class of algorithms from which BPE belongs to. \n",
    "\n",
    "#### Training time (Building a vocabulary)\n",
    "\n",
    "Step 1: Gather a corpus of text.\n",
    "\n",
    "Step 2: Induce a vocabulary by operating on the corpus.\n",
    "\n",
    "- Step 2a: Choose the two symbols that are most frequently adjacent in the training corpus.  \n",
    "- Step 2b: Add new merged symbol to the vocabulary.  \n",
    "- Step 2c: Replace all occurrences of the pair in the corpus with the new merged symbol.  \n",
    "- Step 2d: Repeat (go back to 2a) until the vocabulary reaches a desired size.  \n",
    "\n",
    "> Note: We should retain all the individual characters in the vocabulary. This is because we want to be able to encode arbitrary text. \n",
    "\n",
    "#### Test time (Encode using trained vocabulary)\n",
    "\n",
    "Input language sequence and then the algorithm will break it up into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reference_gpt2.tokenizer.vocab_size)\n",
    "print(reference_gpt2.tokenizer.vocab)\n",
    "\n",
    "sorted_vocab = sorted(list(reference_gpt2.tokenizer.vocab.items()), key=lambda n: n[1])\n",
    "print(sorted_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n",
      "<|endoftext|>\n",
      "[('Ġregress', 50252), ('ĠCollider', 50253), ('Ġinformants', 50254), ('Ġgazed', 50255), ('<|endoftext|>', 50256)]\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.tokenizer.encode(\"<|endoftext|>\"))\n",
    "print(reference_gpt2.tokenizer.decode(50256))\n",
    "print(sorted_vocab[-5:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization is a Headache\n",
    "\n",
    "Whether a word begins with a capital or space matters.  \n",
    "Arithmetic is a mess: Length is inconsistent\n",
    "\n",
    "Play around with the OpenAI tokenizer [here](https://platform.openai.com/tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'But', 'ter', 'fly']\n",
      "['<|endoftext|>', ' Butterfly']\n",
      "['<|endoftext|>', ' butterfly']\n",
      "['<|endoftext|>', 'but', 'ter', 'fly']\n",
      "['<|endoftext|>', '12', '34', '+', '5', '678', '=', '123', '45', '678']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_tokens(\"Butterfly\"))\n",
    "print(reference_gpt2.to_str_tokens(\" Butterfly\"))\n",
    "print(reference_gpt2.to_str_tokens(\" butterfly\"))\n",
    "print(reference_gpt2.to_str_tokens(\"butterfly\"))\n",
    "\n",
    "print(reference_gpt2.to_str_tokens(\"1234+5678=12345678\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<|endoftext|> is a special token that is pre-pended to every sequence. It's a special token that indicates the start of a sequence.\n",
    "\n",
    "Models tend to do some weird things with the first token in a sequence, so we use this to avoid that. A little hacky but it works. It can be disabled with `preprend_bos=False`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We learn a dictionary of vocab of tokens (sub-words).\n",
    "\n",
    "We try to losslessly convert language to integers by tokenizing it.\n",
    "\n",
    "We convert integers to vectors using a lookup table.  \n",
    "\n",
    "(Note: input to the transformer is sequence tokens not vectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logits - Transformer Outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Goal*: Probability distribution over next tokens. For every prefix of the sequnce - given n tokens, we make n next token predictions\n",
    "\n",
    "*Problem*: How to convert a vector to a probability distribution?\n",
    "\n",
    "*Answer*: Use a softmax ($$x_i \\to \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$). The exponential makes everything positive and the normalization makes it sum to 1.\n",
    "\n",
    "So the model outputs a tensor of logits, one vector of size $d_{vocab}$ for each input token."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Generation\n",
    "\n",
    "Convert to tokens\n",
    "\n",
    "shape = batch x position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,    40,   716,   281,  1960,   382, 19741,    11,   875, 12342,\n",
      "            12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,    13]],\n",
      "       device='mps:0')\n",
      "['<|endoftext|>', 'I', ' am', ' an', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.']\n",
      "torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an autoregressive, decoder-only, GPT-2 style transformer.\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text)\n",
    "\n",
    "# Print tokens and their string representations\n",
    "print(tokens)\n",
    "print(reference_gpt2.to_str_tokens(tokens))\n",
    "\n",
    "# shape = (batch_size, sequence_length)\n",
    "print(tokens.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Map tokens to logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "tokens = tokens.to(device)\n",
    "\n",
    "# run_with_cache means cache all intermediate activations, we will view these later\n",
    "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
    "\n",
    "# shape = (batch_size, sequence_length, vocab_size)\n",
    "print (logits.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Convert the logits to a distribution with the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = logits.log_softmax(dim=-1)\n",
    "probs = logits.softmax(dim=-1)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most likely next token for each prefix?\n",
    "\n",
    "Output of the form:\n",
    "(token, next token prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', '\\n'),\n",
       " ('I', \"'m\"),\n",
       " (' am', ' a'),\n",
       " (' an', ' avid'),\n",
       " (' aut', 'od'),\n",
       " ('ore', 'sp'),\n",
       " ('gressive', ','),\n",
       " (',', ' and'),\n",
       " (' dec', 'ently'),\n",
       " ('oder', '-'),\n",
       " ('-', 'driven'),\n",
       " ('only', ','),\n",
       " (',', ' and'),\n",
       " (' G', 'IM'),\n",
       " ('PT', '-'),\n",
       " ('-', 'only'),\n",
       " ('2', '.'),\n",
       " (' style', ','),\n",
       " (' transformer', '.'),\n",
       " ('.', ' I')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(reference_gpt2.to_str_tokens(reference_text), reference_gpt2.tokenizer.batch_decode(logits.argmax(dim=-1)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314:  I\n"
     ]
    }
   ],
   "source": [
    "# Map distribution to a token\n",
    "last_token = logits[0, -1] # (batch -> 0, sequence index -> -1, vocab -> all)\n",
    "next_token = last_token.argmax() # take the highest probability token\n",
    "\n",
    "# Print the next token integer and corresponding string\n",
    "print(f\"{next_token.item()}: {reference_gpt2.tokenizer.decode(next_token)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach the following code to the end of the input, re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Input: tensor([[50256,    40,   716,   281,  1960,   382, 19741,    11,   875, 12342,\n",
      "            12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,    13,\n",
      "           314]], device='mps:0')\n",
      "New Input: <|endoftext|>I am an autoregressive, decoder-only, GPT-2 style transformer. I\n",
      "New Shape:  torch.Size([1, 21, 50257])\n",
      "423:  have\n"
     ]
    }
   ],
   "source": [
    "# Clone and detach the next_token to ensure it's not connected to any computation graph\n",
    "cloned_next_token = next_token.clone().detach()\n",
    "\n",
    "# Move the cloned_next_token to the same device as the original tokens and set its data type to int64\n",
    "cloned_next_token_device = cloned_next_token.to(device=device, dtype=torch.int64)\n",
    "\n",
    "# Add two singleton dimensions to the cloned_next_token tensor to match the dimensions of the tokens tensor\n",
    "reshaped_next_token = cloned_next_token.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Concatenate the reshaped_next_token tensor along the last dimension (-1) with the original tokens tensor\n",
    "next_tokens = torch.cat([tokens, reshaped_next_token], dim=-1)  # (batch -> 0, sequence -> n+1)\n",
    "\n",
    "# Run the concatenated tokens through the GPT-2 model to get new logits\n",
    "new_logits = reference_gpt2(next_tokens)\n",
    "\n",
    "# Print the new input tokens\n",
    "print(\"New Input:\", next_tokens)\n",
    "\n",
    "# Decode the new input tokens to string using the tokenizer\n",
    "print(\"New Input:\", reference_gpt2.tokenizer.decode(next_tokens[0]))\n",
    "print(\"New Shape: \", new_logits.shape)\n",
    "\n",
    "# Predict the next token using the new logits \n",
    "next_token = new_logits[0, -1].argmax()\n",
    "\n",
    "# Print the next token integer and corresponding string\n",
    "print(f\"{next_token.item()}: {reference_gpt2.tokenizer.decode(next_token)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "**Takes in language, predicts the next token for each prefix in a causal way.**  \n",
    "> Transformers are sequence opertaion models. They take in a seqence, do processing on each token in parallel, and then use attention to move information between tokens. \n",
    "\n",
    "We convert language to a sequence of integers with a tokenizer.\n",
    "\n",
    "We convert integers to vectors with a lookup table.\n",
    "\n",
    "Output is a vector of logits for each input token. We convert these to a probability distribution with a softmax and then convert this to a token either by taking the largest token or by sampling from the distribution.\n",
    "\n",
    "We append this to the input and run again generate more text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mech-interp-HQc0oeTw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
